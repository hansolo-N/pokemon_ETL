{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <sub> Pokemon Notebook - Amazon Athena </sub>\n",
    "### analyzing first generation pokemon and berries from the pokemon world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: I did this notebook inside amazon athena notebook editor, as such apache spark environment was already preconfigured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pokemon data into spark df\n",
    "\n",
    "pokemon_df = spark.read.csv('s3://nasrsolobucket/raw_pokemon/pokemon_data.csv',header=True,inferSchema=True)\n",
    "\n",
    "#read pokemon types data into spark df\n",
    "\n",
    "pokemon_types_df = spark.read.csv('s3://nasrsolobucket/pokemon_types_gen1/pokemon_generation_1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnessecary columns\n",
    "pokemon_df = pokemon_df.drop('_c0')\n",
    "\n",
    "pokemon_types_df = pokemon_types_df.drop('Name')\n",
    "\n",
    "\n",
    "# combine dfs\n",
    "combined_df = pokemon_df.join(pokemon_types_df,on=\"id\",how=\"inner\")\n",
    "\n",
    "#display combined df\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print combined pokemon schema\n",
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert weight column to kilograms metric\n",
    "\n",
    "combined_df = combined_df.withColumn('weight',combined_df.weight/10)\n",
    "\n",
    "#convert height column to metres metric\n",
    "\n",
    "combined_df = combined_df.withColumn('height',combined_df.height/10)\n",
    "\n",
    "combined_df.show()\n",
    "\n",
    "combined_df = combined_df.withColumn(\"id\", combined_df[\"id\"].cast(\"integer\"))\n",
    "\n",
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find lightest pokemon(s) in terms of weight\n",
    "\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "min_weight = combined_df.agg(min(\"weight\").alias(\"min_weight\")).collect()[0][\"min_weight\"]\n",
    "\n",
    "min_weight_rows = combined_df.filter(combined_df[\"weight\"] == min_weight)\n",
    "\n",
    "rows = min_weight_rows.collect()\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    print(\"pokemon: {},weight(Kg):{}\".format(row['name'],row['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find heaviest pokemon(s) in terms of weight\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "max_weight = combined_df.agg(max(\"weight\").alias(\"max_weight\")).collect()[0][\"max_weight\"]\n",
    "\n",
    "max_weight_row = combined_df.filter(combined_df[\"weight\"] == max_weight)\n",
    "\n",
    "rows = max_weight_row.collect()\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    print(\"pokemon: {}, weight(Kg):{}\".format(row['name'],row['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find tallest pokemon(s) in terms of height\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "max_height = combined_df.agg(max(\"height\").alias(\"max_height\")).collect()[0][\"max_height\"]\n",
    "\n",
    "max_height_row = combined_df.filter(combined_df[\"height\"] == max_height)\n",
    "\n",
    "rows = max_height_row.collect()\n",
    "\n",
    "for row in rows:\n",
    "    \n",
    "    print(\"pokemon: {}, height(metres):{}\".format(row['name'],row['height']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the 5 highest pokemon with base_experience recieved from defeating them\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "combined_df.sort(desc('base_experience')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group pokemon by type 1\n",
    "combined_df.groupBy(combined_df['Type 1']).agg({\"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter pokemon data for pokemon who have two types\n",
    "filtered_types = combined_df.filter(combined_df['Type 2'].isNotNull())\n",
    "\n",
    "filtered_types.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to s3 bucket\n",
    "combined_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"s3://nasrsolobucket/cleaned_pokemon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pokemon_data.csv into spark df\n",
    "\n",
    "berries_df = spark.read.csv('s3://nasrsolobucket/raw_berries/pokeBerry_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berries_df =berries_df.drop('_c0')\n",
    "\n",
    "berries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print berries schema\n",
    "\n",
    "berries_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average growth time it takes for a tree to grow one stage in hours in hours \n",
    "\n",
    "berries_df.agg({\"growth_time\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of berries that can grow on one tree at a time\n",
    "\n",
    "berries_df.agg({\"max_harvest\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find berries with the biggest size\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "biggest_berries = berries_df.agg(max(\"size\").alias(\"max_size\")).collect()[0][\"max_size\"]\n",
    "\n",
    "biggest_berries_rows = berries_df.filter(berries_df[\"size\"] == biggest_berries)\n",
    "\n",
    "rows = biggest_berries_rows.collect()\n",
    "\n",
    "for row in rows:\n",
    "    print(\"berries: {},size(millimetres):{}\".format(row['name'],row['size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the berry or berries which dries out the quickest\n",
    "\n",
    "max_df =berries_df.agg({\"soil_dryness\":\"max\"})\n",
    "\n",
    "max_value_row = max_df.collect()[0]\n",
    "\n",
    "max_value = max_value_row[0]\n",
    "\n",
    "berries = berries_df.filter(berries_df['soil_dryness']==max_value)\n",
    "\n",
    "print('berry which dries out the quickest')\n",
    "berries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write berries data to s3 bucket\n",
    "berries_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"s3://nasrsolobucket/cleaned_berries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
